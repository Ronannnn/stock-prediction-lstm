{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Vader Lexicon with External Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# stock market lexicon\n",
    "stock_lex = pd.read_csv('lexicon_data/stock_lex.csv')\n",
    "stock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\n",
    "stock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n",
    "stock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\n",
    "stock_lex_scaled = {}\n",
    "for k, v in stock_lex.items():\n",
    "    if v > 0:\n",
    "        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n",
    "    else:\n",
    "        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n",
    "\n",
    "# Loughran and McDonald\n",
    "positive = []\n",
    "with open('lexicon_data/lm_positive.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        positive.append(row[0].strip())\n",
    "    \n",
    "negative = []\n",
    "with open('lexicon_data/lm_negative.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entry = row[0].strip().split(\" \")\n",
    "        if len(entry) > 1:\n",
    "            negative.extend(entry)\n",
    "        else:\n",
    "            negative.append(entry[0])\n",
    "\n",
    "final_lex = {}\n",
    "final_lex.update({word:2.0 for word in positive})\n",
    "final_lex.update({word:-2.0 for word in negative})\n",
    "final_lex.update(stock_lex_scaled)\n",
    "final_lex.update(sia.lexicon)\n",
    "sia.lexicon = final_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider headlines and generate sentiment score from \"nasdaq.com\"\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from params import *\n",
    "\n",
    "\n",
    "website = 'http://www.nasdaq.com/symbol/MSFT/news-headlines'\n",
    "def getText(someList):\n",
    "    returnList = []\n",
    "    for i in someList:\n",
    "        returnList.append(i.text)\n",
    "    return returnList\n",
    "\n",
    "dr = webdriver.Chrome('./chromedriver')\n",
    "dr.get(website)\n",
    "titles=[]\n",
    "dates=[]\n",
    "scores =[]\n",
    "next = dr.find_elements_by_class_name('pagination__next')\n",
    "\n",
    "while(next[0].is_enabled()):\n",
    "    texts = getText(dr.find_elements_by_class_name('quote-news-headlines__item-title'))\n",
    "    date = getText(dr.find_elements_by_class_name('quote-news-headlines__date'))\n",
    "    for t,d in zip (texts,date):\n",
    "        titles.append(t)\n",
    "        dates.append(d)\n",
    "        score = sia.polarity_scores(t)['compound']\n",
    "        scores.append(score)\n",
    "    dr.execute_script(\"arguments[0].click();\", next[0])\n",
    "    time.sleep(2)\n",
    "\n",
    "table = pd.DataFrame([titles,dates,scores])\n",
    "table.to_csv(\"Headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a= np.array([dates,scores]).T\n",
    "table = pd.DataFrame(a,columns=['date','score'])\n",
    "table['score'] = pd.to_numeric(table['score'])\n",
    "a = table.groupby(['date']).mean()\n",
    "a.to_csv('MSFT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from params import *\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dr = webdriver.Chrome('./chromedriver')\n",
    "#website = 'https://www.businesstimes.com.sg/search/facebook?page=#main-content'\n",
    "for i in range(1,50):\n",
    "    website = 'https://www.businesstimes.com.sg/search/facebook?page='+str(i)\n",
    "    dr.get(website)\n",
    "    time.sleep(2)\n",
    "    dr.refresh()\n",
    "\n",
    "    page = dr.page_source\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    date_sentiments = {}\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            #link_page = urlopen(url).read()\n",
    "            dr.get(url)\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            #link_page = urlopen(url).read()\n",
    "            dr.get(url)\n",
    "        dr.refresh()\n",
    "        link_page = dr.page_source\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        print(sentiment)\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "#earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_sentiment = {}\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "#earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
